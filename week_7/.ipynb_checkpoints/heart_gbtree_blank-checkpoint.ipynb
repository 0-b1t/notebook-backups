{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"%reload_ext nb_black\";\n",
       "                var nbb_formatted_code = \"%reload_ext nb_black\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_ext nb_black"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ❤️ Heart Disease 🤒\n",
    "\n",
    "The data today is from the [Framingham Heart Study](https://www.framinghamheartstudy.org/).  Below excerpt from [their wikipedia page](https://en.wikipedia.org/wiki/Framingham_Heart_Study):\n",
    "\n",
    "> The Framingham Heart Study is a long-term, ongoing cardiovascular cohort study of residents of the city of Framingham, Massachusetts. The study began in 1948 with 5,209 adult subjects from Framingham, and is now on its fourth generation of participants. Prior to the study almost nothing was known about the epidemiology of hypertensive or arteriosclerotic cardiovascular disease. Much of the now-common knowledge concerning heart disease, such as the effects of diet, exercise, and common medications such as aspirin, is based on this longitudinal study.\n",
    "\n",
    "### Warm-up 🥵\n",
    "\n",
    "Warm-up warm-ups\n",
    "* Describe what boosting is.\n",
    "* How do random forests avoid overfitting?\n",
    "\n",
    "Actual warm-up\n",
    "* How do we use residuals in gradient boosted trees?\n",
    "* How do we avoid overfitting in gradient boosted trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import and EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    make_scorer,\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# p much in practice:\n",
    "# *if you want to use GradientBoostingClassifier\n",
    "#     * use XGBClassifier instead\n",
    "# *if you want to use GradientBoostingRegressor\n",
    "#     * use XGBRegressor instead\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_vif(x):\n",
    "    \"\"\"Utility for checking multicollinearity assumption\n",
    "    \n",
    "    :param x: input features to check using VIF. This is assumed to be a pandas.DataFrame\n",
    "    :return: nothing is returned the VIFs are printed as a pandas series\n",
    "    \"\"\"\n",
    "    # Silence numpy FutureWarning about .ptp\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        x = sm.add_constant(x)\n",
    "\n",
    "    vifs = []\n",
    "    for i in range(x.shape[1]):\n",
    "        vif = variance_inflation_factor(x.values, i)\n",
    "        vifs.append(vif)\n",
    "\n",
    "    print(\"VIF results\\n-------------------------------\")\n",
    "    print(pd.Series(vifs, index=x.columns))\n",
    "    print(\"-------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = \"https://docs.google.com/spreadsheets/d/1Tx7KJ7iW8IkiU-aERYFXsKvDsbFJbr80POW_2DyuYGQ/export?format=csv\"\n",
    "heart = pd.read_csv(data_url)\n",
    "heart = heart.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do basic EDA to get familiar with this heart data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_cols = [\n",
    "    \"male\",\n",
    "    \"currentSmoker\",\n",
    "    \"BPMeds\",\n",
    "    \"prevalentStroke\",\n",
    "    \"prevalentHyp\",\n",
    "    \"diabetes\",\n",
    "]\n",
    "\n",
    "num_cols = [\n",
    "    \"age\",\n",
    "    \"education\",\n",
    "    \"cigsPerDay\",\n",
    "    \"totChol\",\n",
    "    \"sysBP\",\n",
    "    \"diaBP\",\n",
    "    \"BMI\",\n",
    "    \"heartRate\",\n",
    "    \"glucose\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do we have balanced classes?  If our model gets 85% accuracy, should we consider that good?\n",
    "* Calculate percentages of each class using `value_counts` and the `normalize` argument\n",
    "* Show a bar plot of the counts of each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart[\"TenYearCHD\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(heart[\"TenYearCHD\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize our data with respect to our target variable, `'TenYearCHD'`.  We actually have a lot of categorical variables here that are already encoded as numbers for us. We might consider re-encoding education, but it's already encoded as ordinal, let's keep it as is and come back if we think it will help.\n",
    "\n",
    "However, it might make more sense to visualize these as categorical rather than continuous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_cols = [\n",
    "    \"male\",\n",
    "    \"BPMeds\",\n",
    "    \"prevalentStroke\",\n",
    "    \"prevalentHyp\",\n",
    "    \"diabetes\",\n",
    "]\n",
    "\n",
    "\n",
    "num_cols = [\n",
    "    \"age\",\n",
    "    \"cigsPerDay\",\n",
    "    \"totChol\",\n",
    "    \"diaBP\",\n",
    "    \"BMI\",\n",
    "    \"heartRate\",\n",
    "    \"glucose\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What's an appropriate chart type to plot our categorical variables with our categorical target variable?\n",
    "* Write a `for` loop to iterate over the categorical column names (in `bin_cols`)\n",
    "* Show a plot of `'TenYearCHD'` with each of the categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for col in bin_cols:\n",
    "    perc_chd = heart[[\"TenYearCHD\", col]].groupby(col).mean()\n",
    "    display(perc_chd)\n",
    "\n",
    "    sns.countplot(hue=\"TenYearCHD\", x=col, data=heart)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What's an appropriate chart type to plot our continuous variables with our categorical target variable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in num_cols:\n",
    "    sns.boxplot(\"TenYearCHD\", col, data=heart)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Perform a train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sysBP and currentSmoker dropped based on VIF\n",
    "# sysBP redundant with diaBP\n",
    "# currentSmoker redundant with cigsPerDay\n",
    "X = heart.drop(columns=[\"TenYearCHD\", \"sysBP\", \"currentSmoker\"])\n",
    "y = heart[\"TenYearCHD\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_vif(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Define a `ColumnTransformer` to scale the numeric columns\n",
    "   * Leave the remaining columns untouched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_cols = [\n",
    "    \"male\",\n",
    "    \"BPMeds\",\n",
    "    \"prevalentStroke\",\n",
    "    \"prevalentHyp\",\n",
    "    \"diabetes\",\n",
    "]\n",
    "\n",
    "\n",
    "num_cols = [\n",
    "    \"age\",\n",
    "    \"cigsPerDay\",\n",
    "    \"totChol\",\n",
    "    \"diaBP\",\n",
    "    \"BMI\",\n",
    "    \"heartRate\",\n",
    "    \"glucose\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fmt: off\n",
    "preprocessing = ColumnTransformer([\n",
    "    ____\n",
    "], ____)\n",
    "# fmt: on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Define a `Pipeline` with:\n",
    "    * the `ColumnTransformer` preprocessing as the first step\n",
    "    * an `XGBClassifier` as the second step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fmt: off\n",
    "pipeline = Pipeline([\n",
    "    ____,\n",
    "    ____\n",
    "])\n",
    "# fmt: on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Fit the pipeline to the training data with the default params\n",
    "\n",
    "\n",
    "* What is the overall accuracy?\n",
    "* Are we overfitting?\n",
    "* Is this a good accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "train_score = pipeline.score(X_train, y_train)\n",
    "test_score = pipeline.score(X_test, y_test)\n",
    "\n",
    "print(f\"Train score: {train_score}\")\n",
    "print(f\"Test score: {test_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How are we making mistakes?\n",
    "  * Show a `confusion_matrix` and a `classification report`\n",
    "* In the context of the problem, what kind of mistake is the worst to make?\n",
    "   * Mistake 1: Tell someone they're at risk when they're not\n",
    "   * Mistake 2: Tell someone they're not at risk when they are\n",
    "* Based on that, what number from a `classification_report` are we most interested in?\n",
    "   * Do we want to maximize or minimize this value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try a grid search to see if we get better performance with better parameters.  This is one of the `xgboost` author's thoughts on the hyperparameter tuning.\n",
    "\n",
    "<img src='https://i.stack.imgur.com/9GgQK.jpg' width='70%'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Translation of main parameters of interest:\n",
    "* Name in table - `xgb_parameter_name`\n",
    "\n",
    "---\n",
    "\n",
    "* \\# of Trees - `n_estimators`\n",
    "* Learning Rate - `learning_rate`\n",
    "* Row Sampling - `subsample`\n",
    "* Column Sampling - `colsample_bytree`\n",
    "* Max Tree Depth - `max_depth`\n",
    "\n",
    "---\n",
    "\n",
    "* Set up a grid search using this pictured slide as guidance\n",
    "* What were the best params according to this search?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjusted max_features/max_depth to have smaller grid\n",
    "params = {\n",
    "    \"____\": [0.5, 0.75, 1.0],\n",
    "    \"____\": [0.5, 0.75, 1.0],\n",
    "    \"____\": [5, 7, 10],\n",
    "}\n",
    "\n",
    "n_trees = 100\n",
    "learning_rate = 2 / n_trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_cv = GridSearchCV(pipeline, params, verbose=1, cv=2)\n",
    "pipeline_cv.fit(X_train, y_train)\n",
    "\n",
    "pipeline_cv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How does this affect our performance?\n",
    "* Would we want to deploy this model to predict heart disease?\n",
    "* How can we make it better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score = pipeline_cv.score(X_train, y_train)\n",
    "test_score = pipeline_cv.score(X_test, y_test)\n",
    "\n",
    "print(f\"Train score: {train_score}\")\n",
    "print(f\"Test score: {test_score}\\n\")\n",
    "\n",
    "y_pred = pipeline_cv.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're having a lot of trouble with this class imbalance problem, our model is really biased towards predicting the negative class because most the time it would be correct to do so.\n",
    "\n",
    "There are strategies for dealing with class imbalance, and some common ones that aren't too bad to use are listed out here: https://elitedatascience.com/imbalanced-classes.\n",
    "\n",
    "Let's look into a sampling approach to balance the classes in our training set.\n",
    "\n",
    "* Separate the training data into 2 dataframes:\n",
    "    * One with the majority class\n",
    "    * One with the minority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolating the 2 classes predictors\n",
    "X_train_0 = X_train[y_train == 0]\n",
    "X_train_1 = X_train[y_train == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* How many rows does each have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_0 = X_train_0.shape[0]\n",
    "n_1 = X_train_1.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use sampling to make both sides of the story have the same number of rows\n",
    "    * 'Up sample' with replacement for the minority class\n",
    "    * 'Down sample' without replacement for the majority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = ____"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample majority class to have less observations\n",
    "X_train_0_sample = X_train_0.sample(n, replace=False, random_state=42)\n",
    "\n",
    "# Sample minority class to have less observations\n",
    "X_train_1_sample = X_train_1.sample(n, replace=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Redefine `X_train` and `y_train` with your resampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-combine data (using the downsampled X for majority class)\n",
    "X_train_resample = pd.concat((X_train_1_sample, X_train_0_sample))\n",
    "X_train_resample = X_train_resample.reset_index(drop=True)\n",
    "\n",
    "y_train_resample = np.array([1] * n + [0] * n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Refit the same GridSearchCV object but with this new training data\n",
    "* Print out the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"xgb__subsample\": [0.5, 0.75, 1.0],\n",
    "    \"xgb__max_features\": [0.5, 0.75, 1.0],\n",
    "    \"xgb__max_depth\": [3, 4, 5],\n",
    "}\n",
    "\n",
    "n_trees = 100\n",
    "learning_rate = 2 / n_trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_cv = GridSearchCV(pipeline, params, verbose=1, cv=2)\n",
    "pipeline_cv.fit(X_train_resample, y_train_resample)\n",
    "\n",
    "pipeline_cv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Is the performance better? worse? different at all?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_score = pipeline_cv.score(X_train, y_train)\n",
    "test_score = pipeline_cv.score(X_test, y_test)\n",
    "\n",
    "print(f\"Train score: {train_score}\")\n",
    "print(f\"Test score: {test_score}\\n\")\n",
    "\n",
    "y_pred = pipeline_cv.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
