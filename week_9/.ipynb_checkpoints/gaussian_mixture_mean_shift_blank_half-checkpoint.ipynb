{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2gUM1rBuCD0Q"
   },
   "outputs": [],
   "source": [
    "# %reload_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "uZ0KFum_C0fD",
    "outputId": "8018b7fa-cdf8-4893-a9de-b44f236de69f"
   },
   "outputs": [],
   "source": [
    "# !pip install --upgrade plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "qqhx94S6rxlf"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'plotly'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-7e6f4a3091d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpress\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph_objects\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mgo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'plotly'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# from tqdm.notebook import tqdm\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import KMeans, MeanShift, estimate_bandwidth\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 274
    },
    "id": "6c0V5ezcCD0Z",
    "outputId": "dbe1b6b5-7b6e-4996-95a7-47a860fd86fd"
   },
   "outputs": [],
   "source": [
    "lat_min = 35.71\n",
    "lat_max = 36.55\n",
    "lon_min = -84.55\n",
    "lon_max = -82.37\n",
    "\n",
    "# Potential extra practice:\n",
    "#  * plot all the starbs locations in one color\n",
    "#  * plot my location in a different color\n",
    "adam_lat = 36.3\n",
    "adam_lon = -82.4\n",
    "\n",
    "data_url = \"https://tf-assets-prod.s3.amazonaws.com/tf-curric/data-science/Data%20Sets%20Clustering/starbucks_locations.csv\"\n",
    "starbs = pd.read_csv(data_url)\n",
    "\n",
    "starbs = starbs[starbs[\"Latitude\"].between(lat_min, lat_max)]\n",
    "starbs = starbs[starbs[\"Longitude\"].between(lon_min, lon_max)]\n",
    "starbs.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dAScIXF3CD0c"
   },
   "source": [
    "## üî• Warm up üî•\n",
    "\n",
    "According to the distribution of our filtered data in the `starbs` dataframe.\n",
    "\n",
    "We're most likely to find a starbucks at which of these values of `'Longitude'`?\n",
    "* (A) -81\n",
    "* (B) -83\n",
    "* **(C) -84**\n",
    "* (D) -85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 366
    },
    "id": "khL58H95CD0d",
    "outputId": "90740d73-012e-40a2-f6f3-4939fd832b62"
   },
   "outputs": [],
   "source": [
    "sns.distplot(starbs[\"Longitude\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xmN9w7LfCD0h"
   },
   "source": [
    "According to the distribution of our filtered data in the `starbs` dataframe.\n",
    "\n",
    "We're most likely to find a starbucks at which of these values of `'Latitude'`?\n",
    "* (A) 35\n",
    "* **(B) 36**\n",
    "* (C) 37\n",
    "* (D) 38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 366
    },
    "id": "SjDEi0MxCD0h",
    "outputId": "4bce2c8e-c798-4334-d137-add753912df6"
   },
   "outputs": [],
   "source": [
    "sns.distplot(starbs[\"Latitude\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PzkRHuE9z25r"
   },
   "source": [
    "## Gaussian Mixture Model\n",
    "\n",
    "gaussian distribution = normal distribution\n",
    " \n",
    "### MLE\n",
    "\n",
    "We're given the below data, and we're told that the data is measurements of some snail characteristic üêå.  We want to try and figure out what the *population* distribution looks like.  Remember we just have a *sample*, but the population is all of the snail species we're studying.  Having a good estimate of this population distribution can benefit and guide our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mYvL1j59CD0l"
   },
   "outputs": [],
   "source": [
    "# fmt: off\n",
    "data = [ 8.521, 16.586, 11.154,  3.323, 13.662, 14.649,  6.149,  5.528,\n",
    "        18.871, 11.498,  8.921,  5.776,  7.292,  6.638, 13.321,  7.073,\n",
    "         8.827, 10.375,  1.645, 13.566, 19.846,  6.347,  8.617, 14.462,\n",
    "         4.483, 11.170, 11.322,  5.710, 11.311,  7.672,  9.765, 14.443,\n",
    "        18.360,  9.304, 10.247, 10.955, 14.194,  8.344,  5.783, 12.533,\n",
    "        12.937,  0.846,  4.925,  9.006, 11.443, 16.160, 10.751,  8.513,\n",
    "        23.865, 17.228]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "amFnO7QICD0o"
   },
   "source": [
    "* Before plotting:\n",
    "    * What kind of distribution do you expect this type of data to follow?\n",
    "    * What 'parameters' does that distribution have?\n",
    "        * i.e. these are what you'd need to give `np.random.<distribution_name>()` so it can know what shape of distribution you want random numbers from\n",
    "\n",
    "\n",
    "* Now let's plot a histogram.  Does the shape of the data's distribution support your hypothesis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "b-0P6QE-CD0o",
    "outputId": "4537c722-9863-496e-c6e4-cb0875eebff2"
   },
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "XHMn-4y-CD0t",
    "outputId": "0246b4af-8ee1-4803-bcdb-e608b3c369c7"
   },
   "outputs": [],
   "source": [
    "plt.hist(data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UyVcdJgDCD0v"
   },
   "source": [
    "Remember, we're after the population distribution.\n",
    "\n",
    "* Re-plot the histogram\n",
    "* Add a vertical line at 20 (color it and give it a label for the legend)\n",
    "* Add a vertical line at 10 (color it and give it a label for the legend)\n",
    "* Add a vertical line at  5 (color it and give it a label for the legend)\n",
    "\n",
    "Let's say these are 3 guesses at what the population mean are.  Given our data, which of these is the most *likely*.  Due to the nature of random sampling, it's possible that all 3 of these are valid, but we it's not practical to assume that we got a very unusual random sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "uCg_O2KSCD0z",
    "outputId": "6e12fba3-bd26-45da-8d2a-6f6014cf84be"
   },
   "outputs": [],
   "source": [
    "plt.hist(data)\n",
    "plt.axvline(20, c=\"orange\", label=\"20\")\n",
    "plt.axvline(10, c=\"red\", label=\"10\")\n",
    "plt.axvline(5, c=\"black\", label=\"5\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MWxbV8zICD01"
   },
   "source": [
    "We're engaging in a process called Maximum Likelihood Estimation (MLE).  We're trying to Estimate the population mean based on what's most Likely.  We want our Estimate to have the Maximum Likelihood of being correct.\n",
    "\n",
    "Below is a visualization of us trying to find the population mean via MLE.\n",
    "\n",
    "Note, this is why the mean parameter of `np.random.normal` is called `loc`.\n",
    "\n",
    "<img src='images/mean_mle.gif' width=70%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eOxfH5J5CD02"
   },
   "source": [
    "There's one more parameter we'd need to esimate in order to fully describe our distribution's shape.  This is why this parameter in `np.random.normal` is called `scale` (we scale the width, the height is derived from the width).\n",
    "\n",
    "<img src='images/std_mle.gif' width=60%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "heudzjCUCD02"
   },
   "source": [
    "In this case, the maximum likelihood estimates for the population mean and standard deviation are the equal to the sample mean and sample standard deviations.\n",
    "\n",
    "* Generate 1000 random data points from our estimate popuation distribution\n",
    "* Plot this resulting distribution, compare it to our sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "o1-s4orGCD02",
    "outputId": "ced8b849-acef-49b5-a390-4faed779c7f5"
   },
   "outputs": [],
   "source": [
    "np.mean(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "9mVWq0zDCD08",
    "outputId": "14a267e0-a84e-4ca7-f239-8a67e5f3305c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.std(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 352
    },
    "id": "W45rwfg7CD1S",
    "outputId": "4b8ce1bd-b32e-4eae-aa80-8bc2794e3236"
   },
   "outputs": [],
   "source": [
    "sample = np.random.normal(10.478, 4.702, size=10000)\n",
    "sns.distplot(sample, hist=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucAnUoaCCD1a"
   },
   "source": [
    "### Mixing\n",
    "\n",
    "We just got some new data.  This data was collected from 2 separate species of snails üêå.\n",
    "\n",
    "We know these snails each follow a normal distribution, and we know that they have different means & standard deviations.  Unfortunately, the scientist in the field didn't write down which species each observation is, so we have to try and figure out these 2 separate distributions from a sample of mixed data.\n",
    "\n",
    "If we knew the species labels, we could perform the same MLE process we did above (filter and perform MLE 1 at a time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k3AdzP5CCD1d"
   },
   "outputs": [],
   "source": [
    "# fmt: off\n",
    "data = [11.83984961, 12.37143473, 26.15417807, 28.58500880, 27.70571253,\n",
    "        24.24028217, 18.33611103, 15.28117383, 14.57235710, 18.49006327,\n",
    "        37.83761751, 18.82148403, 36.62430095, 26.61444903, 15.3433858 ,\n",
    "        24.60865873, 31.67437436, 26.08487739, 14.75279305, 25.63485726,\n",
    "        30.44683604, 29.64163292, 14.91536797, 20.48912193, 27.97187397,\n",
    "        11.41235662, 17.90399557, 33.82514212, 17.71352474, 25.98954934,\n",
    "        19.86878159, 26.92304096, 16.25738730, 29.10667734, 31.06548273,\n",
    "        21.14768063, 29.93913722, 25.32381510, 18.98788655, 16.60772929,\n",
    "        25.00896332, 17.41901911, 14.21902871, 27.90108363, 26.99118323,\n",
    "        26.03784060, 31.83483958, 25.73633429, 31.48278996, 24.23683382,\n",
    "        19.24019041, 14.73365444, 27.70687662, 19.82397780, 14.58054905,\n",
    "        22.51116415, 21.31616800, 26.34025573, 28.45094146, 24.61646750]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RZZfxnNRCD1o"
   },
   "source": [
    "Plot the distribution of the data.  Do you think we'll be able to separate out 2 normal distributions from it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 352
    },
    "id": "dNa8orBoCD1p",
    "outputId": "44b39810-55ba-4540-cd80-5a7d6442f0c4"
   },
   "outputs": [],
   "source": [
    "sns.distplot(data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2wt0RMLbCD1r"
   },
   "source": [
    "To figure this out, we'll throw in some initial guesses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lu0i0l-jCD1s"
   },
   "outputs": [],
   "source": [
    "def plot_guess(\n",
    "    data, x1_mean, x1_std, x2_mean, x2_std, n=int(1e6), prob_1=None, prob_2=None\n",
    "):\n",
    "    \"\"\"Helper function for plotting GMM process\"\"\"\n",
    "    sns.distplot(np.random.normal(x1_mean, x1_std, n), hist=False)\n",
    "    sns.distplot(np.random.normal(x2_mean, x1_std, n), hist=False)\n",
    "\n",
    "    if prob_1 is not None and prob_2 is not None:\n",
    "        prob_1 = MinMaxScaler().fit_transform(prob_1)\n",
    "        prob_2 = MinMaxScaler().fit_transform(prob_2)\n",
    "\n",
    "        for x, p1, p2 in zip(data, prob_1, prob_2):\n",
    "            if p1 > p2:\n",
    "                c = \"blue\"\n",
    "                alpha = p1[0]\n",
    "            else:\n",
    "                c = \"orange\"\n",
    "                alpha = p2[0]\n",
    "\n",
    "            plt.scatter(x, 0, alpha=alpha * 0.8, c=c)\n",
    "    else:\n",
    "        plt.scatter(data, [0 for _ in data], c=\"black\")\n",
    "\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "id": "H_X6HWVKCD1x",
    "outputId": "9cba8197-7dde-45d0-e25c-3a0061d94044"
   },
   "outputs": [],
   "source": [
    "plot_guess(data, x1_mean=30, x1_std=6, x2_mean=10, x2_std=6)\n",
    "plt.title(\"initial guess\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AwuRfbnvCD10"
   },
   "source": [
    "Thanks to these distributions, we can now assign a probability (or likelihood) that each point came from each distribution.  Using this, we can split the data into 2 groups:\n",
    "\n",
    "* (1) points more likely to have come from the orange distribution\n",
    "* (2) points more likely to have come from the blue distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "id": "eOaLiVMLCD11",
    "outputId": "680da8ac-5eaa-4ec2-ed5e-a80abefed622"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data, columns=[\"mixture\"])\n",
    "df[\"prob_blue\"] = stats.norm(30, 6).pdf(df[\"mixture\"])\n",
    "df[\"prob_orange\"] = stats.norm(10, 6).pdf(df[\"mixture\"])\n",
    "df[\"label\"] = \"blue\"\n",
    "df.loc[df[\"prob_orange\"] > df[\"prob_blue\"], \"label\"] = \"orange\"\n",
    "\n",
    "plot_guess(\n",
    "    data,\n",
    "    x1_mean=30,\n",
    "    x1_std=6,\n",
    "    x2_mean=10,\n",
    "    x2_std=6,\n",
    "    prob_1=df[[\"prob_blue\"]],\n",
    "    prob_2=df[[\"prob_orange\"]],\n",
    ")\n",
    "plt.title(\"initial guess\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3R3GGxXNCD13"
   },
   "source": [
    "We now have 2 individual groups of data! And just like in the single species example, we can now make a better guess about what each distribution looks like.  We can use this to update our guess of the distribution shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 166
    },
    "id": "gJW97oJuCD14",
    "outputId": "1e64bb63-cc19-43d2-9d13-d0ca8fe160a5"
   },
   "outputs": [],
   "source": [
    "df.groupby(\"label\").agg({\"mixture\": [\"mean\", \"std\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "id": "aHmgtLQECD16",
    "outputId": "0394a45f-498e-404f-f157-e9a8a39ce910"
   },
   "outputs": [],
   "source": [
    "plot_guess(\n",
    "    data,\n",
    "    x1_mean=27.508272,\n",
    "    x1_std=3.867000,\n",
    "    x2_mean=16.238787,\n",
    "    x2_std=2.525888,\n",
    "    prob_1=df[[\"prob_blue\"]],\n",
    "    prob_2=df[[\"prob_orange\"]],\n",
    ")\n",
    "plt.title(\"updated guess\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "id": "tN7tkC0KCD18",
    "outputId": "81ef0e03-b6f7-4e17-96aa-d91c8d3e8907"
   },
   "outputs": [],
   "source": [
    "plot_guess(\n",
    "    data, x1_mean=27.508272, x1_std=3.867000, x2_mean=16.238787, x2_std=2.525888,\n",
    ")\n",
    "plt.title(\"updated guess\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lrrc8anMCD1_"
   },
   "source": [
    "And repeat! This process we just went through goes by the name Gussian Mixture Modeling.  Gaussian is another name for the normal distribution (named after a dude who contributed a lot to math and statistics).\n",
    "\n",
    "Let's see how we can approach this problem with sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "id": "IBxSE1N2CD1_",
    "outputId": "a9c2716e-e9f4-41a1-c219-b86d3830731a"
   },
   "outputs": [],
   "source": [
    "gmm = GaussianMixture(2)\n",
    "gmm.fit(df[[\"mixture\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l1QPTvARCD2C"
   },
   "source": [
    "Here are the means and standard deviations that sklearn settled on after going through that process a couple more iterations.  Note that in this simple case, our 1 iteration algorithm got pretty similar results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 469
    },
    "id": "oocgMfIQCD2C",
    "outputId": "93229fd8-9563-4435-bece-58ea360362a6"
   },
   "outputs": [],
   "source": [
    "x1_mean, x2_mean = gmm.means_\n",
    "x1_std, x2_std = np.sqrt(gmm.covariances_)\n",
    "\n",
    "x1_mean = x1_mean[0]\n",
    "x2_mean = x2_mean[0]\n",
    "x1_std = x1_std[0][0]\n",
    "x2_std = x2_std[0][0]\n",
    "\n",
    "print(f\"Cluster 1 - mean: {x1_mean:.2f}; std: {x1_std:.2f}\")\n",
    "print(f\"Cluster 2 - mean: {x2_mean:.2f}; std: {x2_std:.2f}\")\n",
    "\n",
    "plot_guess(\n",
    "    data,\n",
    "    x1_mean=x1_mean,\n",
    "    x1_std=x1_std,\n",
    "    x2_mean=x2_mean,\n",
    "    x2_std=x2_std,\n",
    "    prob_1=df[[\"prob_blue\"]],\n",
    "    prob_2=df[[\"prob_orange\"]],\n",
    ")\n",
    "plt.title(\"sklearn's guess\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KQzshiHXCD2H"
   },
   "source": [
    "That's cool and all, but our data almost never has just a single column (i.e. this example was *univariate*).  Let's look at a case with multiple variables (i.e. *multivariate*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "6JutmvvlCD2H",
    "outputId": "a6094c9c-f92c-4665-ea7c-66e918a55b9a"
   },
   "outputs": [],
   "source": [
    "lat_lon = starbs[[\"Latitude\", \"Longitude\"]].copy()\n",
    "\n",
    "fig = px.scatter_geo(lat_lon, \"Latitude\", \"Longitude\", scope=\"usa\")\n",
    "fig.update_geos(fitbounds=\"locations\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6sBNV4GWCD2P"
   },
   "source": [
    "We can try and visualize both distributions at once using a couple different methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "9EeV5BFJCD2P",
    "outputId": "a2bb748e-84da-4ffb-e80f-ed2cfec61555"
   },
   "outputs": [],
   "source": [
    "px.scatter(\n",
    "    starbs, \"Longitude\", \"Latitude\", marginal_x=\"histogram\", marginal_y=\"histogram\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "aucj6tK5CD2R",
    "outputId": "b92456e0-f4fb-4edb-dde0-8babd38d58ad",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x = starbs[\"Longitude\"]\n",
    "y = starbs[\"Latitude\"]\n",
    "kernel = stats.gaussian_kde((x, y))\n",
    "\n",
    "x = np.linspace(x.min(), x.max(), 100)\n",
    "y = np.linspace(y.min(), y.max(), 100)\n",
    "\n",
    "z = []\n",
    "for xi in x:\n",
    "    zi = []\n",
    "    for yi in y:\n",
    "        zi.append(kernel((xi, yi))[0])\n",
    "\n",
    "    z.append(zi)\n",
    "\n",
    "z = np.array(z)\n",
    "z.reshape((x.shape[0], y.shape[0]))\n",
    "\n",
    "fig = go.Figure(data=[go.Surface(z=z, x=x, y=y)])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4SHIpSD0CD2W"
   },
   "source": [
    "Just like KMeans, we need to decide up front how many clusters we want the clustering process to find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yAu-thrM0ICo"
   },
   "outputs": [],
   "source": [
    "k = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JZb2i-ihCD2Y"
   },
   "source": [
    "* Create and fit a `GaussianMixture()` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "id": "KWEriFCTCD2Z",
    "outputId": "04c32633-c997-43d2-b70d-c6084a098464"
   },
   "outputs": [],
   "source": [
    "gauss = GaussianMixture(n_components=k, random_state=42)\n",
    "gauss.fit(lat_lon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6DphHLmMCD2b"
   },
   "source": [
    "Let's compare `GaussianMixture()` to `KMeans()`.\n",
    "\n",
    "* Create and fit a `KMeans()` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "id": "nclmcAceCD2c",
    "outputId": "accdf787-0ff0-43f1-8fdf-2cd298821d0f"
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "kmeans.fit(lat_lon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bIn1aOJRCD2e"
   },
   "source": [
    "* Print the resulting centroids from each method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "id": "n6arQSpGCD2f",
    "outputId": "0fb2fea1-9d21-484f-eeea-e68cac89fe9f"
   },
   "outputs": [],
   "source": [
    "gauss.means_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "id": "UMNetyiACD2j",
    "outputId": "70d9d978-69d2-4713-e93f-8905bb3d64a0"
   },
   "outputs": [],
   "source": [
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N21aFwcZCD2n"
   },
   "outputs": [],
   "source": [
    "starbs[\"label\"] = gauss.predict(lat_lon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wWa9kmoeCD2q"
   },
   "outputs": [],
   "source": [
    "# starbs[starbs[\"label\"] == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WqHkS3_rCD2u"
   },
   "source": [
    "* Use the `GaussianMixture.predict_proba()` method.\n",
    "* What are the top 3 observations we are least confident about?\n",
    "* What are the top 3 observations we are most confident about?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "d6HjnWGrFO-q",
    "outputId": "0ccce550-6385-43e4-e9f9-62efe923a587",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prob_df = pd.DataFrame(gauss.predict_proba(lat_lon))\n",
    "prob_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OU5DuIxvCD2z"
   },
   "source": [
    "Visualizing the results comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "bvauiLwZ0m3v",
    "outputId": "446a42da-2e8b-4ecc-fc82-88e99b6ef6a5",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_df = lat_lon.copy()\n",
    "plot_df[\"label\"] = gauss.predict(lat_lon)\n",
    "plot_df = plot_df.sort_values(\"label\")\n",
    "\n",
    "centers_df = pd.DataFrame(gauss.means_, columns=[\"Latitude\", \"Longitude\"])\n",
    "centers_df[\"label\"] = \"Cluster center\"\n",
    "\n",
    "plot_df = pd.concat((plot_df, centers_df), sort=False)\n",
    "\n",
    "fig = px.scatter_geo(\n",
    "    plot_df,\n",
    "    \"Latitude\",\n",
    "    \"Longitude\",\n",
    "    color=\"label\",\n",
    "    scope=\"usa\",\n",
    "    title=\"Gaussian Mixture Model Results\",\n",
    "    hover_name=plot_df.index,\n",
    ")\n",
    "\n",
    "fig.update_geos(fitbounds=\"locations\")\n",
    "fig.show()\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "plot_df = lat_lon.copy()\n",
    "plot_df[\"label\"] = kmeans.labels_\n",
    "plot_df = plot_df.sort_values(\"label\")\n",
    "\n",
    "centers_df = pd.DataFrame(kmeans.cluster_centers_, columns=[\"Latitude\", \"Longitude\"])\n",
    "centers_df[\"label\"] = \"Cluster center\"\n",
    "\n",
    "plot_df = pd.concat((plot_df, centers_df), sort=False)\n",
    "\n",
    "fig = px.scatter_geo(\n",
    "    plot_df,\n",
    "    \"Latitude\",\n",
    "    \"Longitude\",\n",
    "    color=\"label\",\n",
    "    scope=\"usa\",\n",
    "    title=\"KMeans Results\",\n",
    "    hover_name=plot_df.index,\n",
    ")\n",
    "\n",
    "fig.update_geos(fitbounds=\"locations\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HVwJByXZztt_"
   },
   "source": [
    "## Mean-shift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LDjszg1QCD24"
   },
   "source": [
    "* Use `matplotlib` to plot the East TN Starbucks\n",
    "* Label the `x` and `y` axes\n",
    "* Give the plot a title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EnPvZhaqih5R"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oT8TbHT1CD26"
   },
   "source": [
    "* Use `sns.kdeplot()` to show the distribution of Latitude and Longitude separately.\n",
    "* How many clusters do you expect to find if clustering only on one of these variables?\n",
    "* Play with the `bw` parameter of `sns.kdeplot()`.  How does this change how many clusters you expect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 537
    },
    "id": "4xNvhNFzCD28",
    "outputId": "8a3587a3-f802-44d1-edde-fce54bab2618"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "sns.kdeplot(____, ax=axes[0])\n",
    "sns.kdeplot(____, ax=axes[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wO5yT6YPCD2_"
   },
   "source": [
    "* Use `sns.kdeplot()` to show the 2d distribution of Latitude and Longitude.\n",
    "* Add the starbucks locations to the plot.\n",
    "* How many clusters do you expect to find if clustering only on one of these variables?\n",
    "* Play with the `bw` parameter of `sns.kdeplot()`.  How does this change how many clusters you expect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IAAWE0zNybug"
   },
   "outputs": [],
   "source": [
    "sns.kdeplot(____)\n",
    "plt.scatter(____)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MkQ9kkfpCD3T"
   },
   "source": [
    "A slightly different view of the same plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jJfhwe-sCD3Z"
   },
   "outputs": [],
   "source": [
    "bw = 0.1\n",
    "\n",
    "x = starbs[\"Longitude\"]\n",
    "y = starbs[\"Latitude\"]\n",
    "kernel = stats.gaussian_kde((x, y), bw_method=bw)\n",
    "\n",
    "x = np.linspace(x.min(), x.max(), 100)\n",
    "y = np.linspace(y.min(), y.max(), 100)\n",
    "\n",
    "z = []\n",
    "for xi in x:\n",
    "    zi = []\n",
    "    for yi in y:\n",
    "        zi.append(kernel((xi, yi))[0])\n",
    "\n",
    "    z.append(zi)\n",
    "\n",
    "z = np.array(z)\n",
    "z.reshape((x.shape[0], y.shape[0]))\n",
    "\n",
    "fig = go.Figure(data=[go.Surface(z=z, x=x, y=y)])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-dqBBxA-CD3m"
   },
   "source": [
    "* Use `MeanShift()` to cluster the observations\n",
    "* The default `bandwidth` used by `MeanShift()` is calculated using `sklearn.cluster.estimate_bandwidth()` (shown below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NmT9dzkhCD3m"
   },
   "outputs": [],
   "source": [
    "default_bw = estimate_bandwidth(lat_lon)\n",
    "print(f\"Default bandwidth: {default_bw}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BLxIF9rXq_l-"
   },
   "outputs": [],
   "source": [
    "# Define a variable to hold the selected bw for use in plotting later\n",
    "bw = _____\n",
    "\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y6zM-HxrCD3r"
   },
   "source": [
    "* Show the resulting cluster centers.  How many clusters were found?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eISiQxOeCD3s"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EmebbI9JCD3u"
   },
   "source": [
    "* Redo the above plot colored by cluster label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "87AxYSJtCD3u"
   },
   "outputs": [],
   "source": [
    "# seaborn converts all columns that look like numbers to numeric\n",
    "# we dont want a color bar to represent cluster labels as they're purely\n",
    "# categorical. to stop seaborn from thinking of our labels as continuous\n",
    "# we're adding cluster labels as strings and adding an underscore\n",
    "\n",
    "lat_lon[\"label\"] = mean_shift.labels_.astype(str)\n",
    "lat_lon[\"label\"] += \"_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O5fJMUE8CD3x"
   },
   "outputs": [],
   "source": [
    "sns.kdeplot(____)\n",
    "sns.scatterplot(____)\n",
    "\n",
    "plt.title(\"Clustered East TN Starbucks\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p3M2EAG6CD30"
   },
   "source": [
    "#### Extra practice in case we have time:\n",
    "\n",
    "* The nba dataset is loaded & cleaned for you below.\n",
    "* Apply mean shift and interpret the clusters.\n",
    "* Based on your interpretation, are these good clusters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RKRcPc43CD30"
   },
   "outputs": [],
   "source": [
    "data_url = \"https://tf-assets-prod.s3.amazonaws.com/tf-curric/data-science/Data%20Sets%20Clustering/nba_player_seasons.csv\"\n",
    "nba = pd.read_csv(data_url)\n",
    "\n",
    "nba = nba[(nba[\"GS\"] >= 20) & (nba[\"MP\"] >= 10)]\n",
    "nba = nba.dropna().reset_index(drop=True)\n",
    "nba_og = nba.copy()\n",
    "\n",
    "nba = nba[[\"PTS\", \"TRB\", \"TOV\", \"AST\", \"BLK\", \"Age\"]]\n",
    "nba.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w6YXwpF3CD4H"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "gaussian_mixture_mean_shift_blank.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
