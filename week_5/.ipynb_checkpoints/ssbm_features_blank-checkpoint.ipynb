{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smash Bros\n",
    "\n",
    "Today's extra nerdy dataset is stats from a tournament of the game *Super Smash Bros. Melee* for the Nintendo GameCube.  The stats were recorded during the tournament [Smash Summit 2017](https://liquipedia.net/smash/Smash_Summit/Spring_2017), these stats were collected by an older version the [Slippi](https://slippi.gg/faq) project.  The data was collected with this [R script](https://github.com/AdamSpannbauer/twitch_chat/blob/master/r_scripts/slippi_win.R) in 2017 (no promises the script still works).\n",
    "\n",
    "The data contains a player id column, `gamerTag`, and a binary variable indicating if the player won the set: `won`.  The rest of the variables are different statistics counted up throughout the set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "data_url = \"https://raw.githubusercontent.com/AdamSpannbauer/twitch_chat/master/data/slippi_data/ssbm_win.csv\"\n",
    "ssbm = pd.read_csv(data_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the below function to show the biggest offenders for NAs in a `pandas.DataFrame` (i.e. show the top n columns by number or percent of `np.nan` values in the column).\n",
    "\n",
    "* Give the `n` parameter a default value of `5`\n",
    "* Give the `rate` parameter a default boolean value\n",
    "* Fill in the blanks to match what the comments are saying"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_top_missing(df, n, rate):\n",
    "    \"\"\"Utility to print most 'sparse' columns by missing values\"\"\"\n",
    "\n",
    "    # Create a boolean df indicating which values are np.nan\n",
    "    is_na_df = ____\n",
    "\n",
    "    # Aggregate the df:\n",
    "    # * if rate is true, then calculate what percent of values are nan\n",
    "    # * if rate is false, then calculate the count of nans\n",
    "    if rate:\n",
    "        missing_stats = ____\n",
    "    else:\n",
    "        missing_stats = ____\n",
    "\n",
    "    top_missing = missing_stats.sort_values(ascending=False)\n",
    "\n",
    "    # We might consider to return or print here\n",
    "    # If we wanted to work with the output values, we'd want\n",
    "    # to use return.  I just wanted to see a display of the\n",
    "    # output so I chose print instead.\n",
    "    print(top_missing.head(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you followed ALL the prompts for the function\n",
    "# this code should run without error. If you get an \n",
    "# error about \"missing required positional arguments\"\n",
    "# then you need to check that you set default values \n",
    "# for n and rate.\n",
    "\n",
    "show_top_missing(ssbm)\n",
    "ssbm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create some visualization(s) in light of the goal (predicting win).  Based on the visualization, what do you think will be a good predictor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In most machine learning contexts, we will isolate our input features from our target variable.  Player ID won't be much use since we want our model to predict wins for every player, not just the 16 players in our data set.\n",
    "\n",
    "* So we'll drop `gamerTag` & `won` and assign it to `X`\n",
    "    * This `X` is a conventional way to specify that this variable is our input features\n",
    "* It's also conventional to put your target into a variable named `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store every column but 'gamerTag' and 'won' in X\n",
    "X = ____\n",
    "\n",
    "# Store what you want to predict ('won') in y\n",
    "y = ____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead and try to build our first model to predict this win.  Since the topic at hand is feature selection, we won't really discuss the model being used (yet); we'll just use it as a way to grade our features.  Note, this model building is also not following the best practice of doing a train/test split to evaluate accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "# In practice score on a test set\n",
    "model.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see we ran into a `ConvergenceWarning`(note that it tells us some possible solutions to this issue).  Despite this issue we ended up predicting with ~95% accuracy.\n",
    "\n",
    "*Note, again, this is a **bad way to evaluate a model**, we should be testing our model on data the model has never seen before.*\n",
    "\n",
    "## Feature Selection\n",
    "\n",
    "We might be able to avoid this warning by narrowing down our features.  Narrowing down features has a lot of benefits, including:\n",
    "* Aiding in computation issues (RE:above warning)\n",
    "* Aiding in accuracy\n",
    "    * Can be a direct result from helping first point\n",
    "* Aiding in interpretability\n",
    "    * A lot of times you're not only interested in predicting something accurately, but also explaining why you're good at making these predictions.  For example, in a business context you might want to accurately predict customer churn, but you're management would also really like to know what is indicating that a customer is going to churn.  With an easily interpretable model you can show the impact each input feature has on a customer's likelihood to churn.  With a lot of predictors this gets harder to parse out.\n",
    "    * There's also an issue known as [multicollinearity](https://en.wikipedia.org/wiki/Multicollinearity).  This intuitively means that some of your input features are encoding the same information (like having a feature for `temperature_celsius` & one for `temperature_farenheit`).  When your model has multiple variables representing the same thing it can confuse the model, the model might still be accurate, but it will have more trouble explaining to you why it's accurate.\n",
    "    \n",
    "In this case, a way we can smartly select a subset of predictors is with `SelectKBest` with the `f_classif` method.  If you remember, ANOVA outputs a value called $F$, this is what the `f` in `f_classif` represents.  All we're doing here is running an ANOVA for each of our variables and target.\n",
    "\n",
    "Our target variable is a category and all our predictors are numeric, so for each numeric feature we run an ANOVA using the target as the groups.  Below is an example using the feature `numKillingPunishes`.  To start we might visualize the differences between the 2 groups.  We see some difference, but we can quantify this difference using the `stats.f_oneway` function we've used before to perform an ANOVA.  We end up with an $F$ value of about `23.8`.\n",
    "\n",
    "Note, we didn't really check for normality here.  ANOVA is fairly robust against violations of the normality assumption. Also, in the case variable selection, we aren't trying to declare any significant difference, we're just looking for potentially useful differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform an ANOVA for the `numKillingPunishes` feature grouped by `won`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(\"won\", \"numKillingPunishes\", data=ssbm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "won = ssbm[ssbm[\"won\"] == 1]\n",
    "lost = ssbm[ssbm[\"won\"] == 0]\n",
    "\n",
    "stats.f_oneway(won[\"numKillingPunishes\"], lost[\"numKillingPunishes\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we've done an ANOVA for one of our variables, we can use `SelectKBest` from `sklearn` to do the rest.  It will:\n",
    "\n",
    "* run the same ANOVA\n",
    "* store all the values of $F$\n",
    "* keep only the features with the top $k$ values of $F$\n",
    "\n",
    "We have to select the value of $k$, and there's not a general rule of thumb on what it should be. Below, I chose 5 cause I wanted to it to choose the 5 best because ¯\\\\_(ツ)_/¯.\n",
    "\n",
    "From the output, we can see the scores for each ANOVA that was run with `f_classif`, and we can confirm that we see the same value for `numKillingPunishes` as we did in our one variable example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use `SelectKBest` and `f_classif` to choose the best 5 predictors of `won`\n",
    "* Put these best predictors back into a dataframe with the original names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = ____(____, k=5)\n",
    "\n",
    "# Use `.fit()` method so the selector can 'learn' from our data\n",
    "selector.____\n",
    "\n",
    "# Use `.transform()` method so the selector can apply\n",
    "# what it learned in `.fit()`\n",
    "k_best = selector.____\n",
    "\n",
    "# We can see/rank which features were the best\n",
    "score_df = pd.DataFrame({\"feature\": X.columns, \"f_score\": selector.scores_})\n",
    "score_df = score_df.sort_values(\"f_score\", ascending=False)\n",
    "print(score_df.head())\n",
    "\n",
    "# We can put back into a dataframe to see column names\n",
    "best_df = pd.DataFrame(k_best, columns=ssbm.columns)\n",
    "best_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could now move back to modeling this data using the same code as before, but now providing only the 5 'best' features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(k_best, y)\n",
    "# In practice score on a test set\n",
    "model.score(k_best, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this restricted set, we didn't get a `ConvergenceWarning`. We do see a drop in accuracy, but this might actually be a sign that our first model was [*overfitting*](https://en.wikipedia.org/wiki/Overfitting).  Again, we can't take much from the score on our training data (see note below, and above, and everywhere).\n",
    "\n",
    "However, we didn't lose too much accuracy despite deleting 19 of our input columns.  This shows that we likely have a lot of 'dead weight' in our features, and we can likely predict the outcome without having all 24 features to wade through (which can be a burden for both our model and us during interpretation).\n",
    "\n",
    "*Note, again, this is a **bad way to evaluate a model**, we should be testing our model on data the model has never seen before.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA\n",
    "\n",
    "In PCA (principle components anaylsis) we capture the variance of our data into 'components'.  What does that mean? Well, we'll be predicting our outcome based on how the other variables' values change (or vary).  So our model is really wanting to pick up on how the input features vary in relation to the outcome we're predicting.  PCA captures all this 'varying' in our data by combining our features, and it can capture this variance with less features than we started with.\n",
    "\n",
    "Since we're really focused on how our features are varying, we `scale` them before doing PCA. Remember that `scale` will transform our inputs so that their means are 0 and their standard deviations are 1.  With all these standard deviations set to 1, none of the features will overpower the others in terms of their variance.  For example, the `totalComboDamage` has a variance of 188387.3 and `openingsPerKill` has a variance of 2.8, this is a big gap.  PCA is trying to maximize the amount of variation captured, and an easy way to shortcut this would be to focus in on features with larger variances; we want to put all our features on equal ground, and so we scale.\n",
    "\n",
    "Below we see the effects of scaling on our features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use the `StandardScaler()` to scale the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Feature variances before scaling:\")\n",
    "print(X.var().sort_values(ascending=False).head())\n",
    "\n",
    "scaler = ____\n",
    "\n",
    "# 'learn' from the data\n",
    "scaler.____\n",
    "\n",
    "# apply what the scaler learned\n",
    "scaled_X = scaler.____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can move onto performing PCA.  To do this we can use `sklearn.decomposition.PCA`, and the main thing left for us to decide is how many components.  The reading focuses in on eigenvalues, here we'll use the variance explained and a scree plot.  I'd say the 'elbow' that we typically look for in a scree plot occurs after our 4th component.  So we might choose to set `n_components` to 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create a `PCA` object and apply it to the scaled data\n",
    "* Plot the variance explained by each component\n",
    "* Create a correlation heatmap of the prinicple components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "\n",
    "# 'learn' from the data\n",
    "pca.____\n",
    "\n",
    "# apply what the pca object learned\n",
    "pca_X = pca.____(scaled_X)\n",
    "\n",
    "# What attribute holds the variance explained?\n",
    "plt.plot(____, marker=\"o\")\n",
    "plt.title(\"Variance explained by each component\")\n",
    "plt.show()\n",
    "\n",
    "sns.heatmap(____, vmin=-1, vmax=1)\n",
    "plt.title(\"Correlation between components\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use what we learned from the scree plot to choose an appropriate number of components to use for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(____)\n",
    "\n",
    "# 'learn' from the data\n",
    "pca.____\n",
    "\n",
    "# apply what the pca object learned\n",
    "pca_X = pca.____(scaled_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(pca_X, y)\n",
    "# In practice score on a test set\n",
    "model.score(pca_X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we went ahead and refit the model using our first 4 principle components.  We don't get a warning about convergence, and we seem to have a pretty decent fit (as judged by our bad scoring practice).  One thing to note about the model that was built.  It's going to be very hard to interpret.  We'll be able to see what effect each principle component has on the target variable.  This isn't the most intuitive thing to grasp, and it's even harder to explain to someone with a non-technical background.  PCA can sometimes be a positive in terms of model accuracy, but can also be a negative in terms of loss of iterpretability and understanding.\n",
    "\n",
    "*Note, again, again, this is a **bad way to evaluate a model**, we should be testing our model on data the model has never seen before.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
