{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Significance\n",
    "\n",
    "Slides [aquÃ­](https://docs.google.com/presentation/d/10jV-CsgaaGv2F7H_gZnkz5_eVyVmqKLrc1rf26e2aDY/edit#slide=id.p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.stats.api as sms\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm Up! ðŸ¥µ\n",
    "\n",
    "### ðŸ¥µ#1\n",
    "\n",
    "* How do we evaluate scedasticity and why is it important to make sure this assumption is met in linear modeling?\n",
    "  * What even is scedasticity? `________________`\n",
    "  * Do we want to have homoscedasticity or heteroscedasticity `________________`\n",
    "  * In the below cell... plot `X` with `y1` and `X` with `y2`.  Which one do you think is homoscedastic? heteroscedastic? `________________`\n",
    "  * What about something more formal than this eyeball test?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed for reproducible random stuff\n",
    "# Just generating some data to play with\n",
    "np.random.seed(42)\n",
    "\n",
    "n = 100\n",
    "X = np.arange(n)\n",
    "y1 = X + np.random.normal(0, 10, size=n)\n",
    "y2 = X + np.random.normal(0, X ** 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot X with y1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot X with y2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What about something more formal than this eyeball test?\n",
    "# Fit a model for X to predict y1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a model for X to predict y2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Documenation: https://www.statsmodels.org/dev/generated/statsmodels.stats.diagnostic.het_breuschpagan.html\n",
    "# Null hypothesis is homoscedastic\n",
    "_, p1, _, _ = sms.het_breuschpagan(____, X_const)\n",
    "_, p2, _, _ = sms.het_breuschpagan(____, X_const)\n",
    "\n",
    "print(p1)\n",
    "print(p2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ¥µ #2\n",
    "\n",
    "* How do we evaluate for multicollinearity and why is this assumption important?\n",
    "  * What even is multicollinearity? `________________`\n",
    "  * Do we want to have multicollinearity or no multicollinearity? `________________`\n",
    "  * In the below cell... Does `X` contain significant multicollinearity?\n",
    "  * When creating dummies/one-hot-encoded variables, what do we need to do to avoid multicolinearararity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed for reproducible random stuff\n",
    "np.random.seed(42)\n",
    "\n",
    "n = 20\n",
    "x1 = np.arange(n)\n",
    "x2 = np.random.normal(size=n)\n",
    "x3 = 2 * np.arange(n) + np.random.normal(0, 1, n)\n",
    "X = pd.concat((pd.Series(x) for x in [x1, x2, x3]), axis=1)\n",
    "X.columns = [\"x1\", \"x2\", \"x3\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does X contain significant multicollinearity?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## t-tests again!! ðŸŽ‰\n",
    "\n",
    "I can feel the excitement bubbling up in you all just writing this... I know, I missed them too.\n",
    "\n",
    "Let's look more at this $t$ distribution to start.  We didn't really do that before.\n",
    "\n",
    "* Generate a t distribution with `np.random._______`.\n",
    "    * Use a large-ish sample size\n",
    "    * Play with the value of the degrees of freedom parameter (From the slide: \"more degrees of freedom means closer to the normal distribution\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed for reproducible random stuff\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate a t distribution with an np.random function and plot\n",
    "n = 1000\n",
    "\n",
    "x = ______\n",
    "plt.hist(x)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What's the null hypothesis in a t-test?\n",
    "* When do we reject this null hypothesis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the average heights (in cm) by sex in USA according to [this big ole report](https://www.cdc.gov/nchs/data/series/sr_03/sr03_039.pdf) that I really found from [this wikipedia page](https://en.wikipedia.org/wiki/Average_human_height_by_country); the info also gives the sample size and standard deviation.\n",
    "\n",
    "* Generate random normal samples using this information.\n",
    "* Plot overlaid histograms of these samples you generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "male_n = 5232\n",
    "male_avg = 175.3\n",
    "male_sd = 15.2\n",
    "\n",
    "female_n = 5547\n",
    "female_avg = 161.5\n",
    "female_sd = 15.2\n",
    "\n",
    "# Generate random normal samples using this information.\n",
    "np.random.seed(42)\n",
    "female_height = _____\n",
    "male_height = _____\n",
    "\n",
    "# Plot overlaid histograms of these samples you generated.\n",
    "_____\n",
    "_____\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the histograms (and the average) men are taller.  Is this difference significant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we had biased samples?? What is a biased sample??\n",
    "\n",
    "Let's say we took a random sample of women and only ended up with WNBA players (not likely with a random sample, but the beauty of randomness is that it's possible, some things are just unlikely).  Below is the average height for WNBA players according to [reference.com](https://www.reference.com/world-view/average-height-wnba-player-a3cf4bccebffecfb); also is the number of WNBA players according to the [New York Times](https://www.nytimes.com/2018/05/05/sports/wnba-los-angeles-sparks.html).\n",
    "\n",
    "Let's say our sample of men ended up with only marathon runners.  Below is the average height for top 100 male marathon runners according to [runnersworld.com](https://www.runnersworld.com/news/a20855134/great-marathoners-over-six-feet-tall-are-rare/).  \n",
    "\n",
    "Use the same standard deviations from before and:\n",
    "* generate some more normal distributions\n",
    "* re-plot\n",
    "* re-ttest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnba_avg = 180.3\n",
    "wnba_n = 144\n",
    "\n",
    "marathon_avg = 169.92\n",
    "marathon_n = 100\n",
    "\n",
    "# generate some more normal distributions\n",
    "\n",
    "# re-plot\n",
    "\n",
    "# re-test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We conclude the opposite! Kinda forced us into this.. but! This is the whole idea about the p-value and the typical 5% cutoff.  The p-value is trying to help us differentiate from results that occur purely from chance related to sampling.  AKA every p-value is trying to protect against the chance that you're comparing WNBA players and marathoners when you want to be comparing women and men.  AKA a p-value is the probability that random chance generated differences that you're seeing in the data.\n",
    "\n",
    "When we reject the null at the 5% level we're still saying there's a 5% chance these results happened due to sampling bias.  Typically 5% is pretty good in practice; here we forced the issue and got a reallllly unlikely sample to happen randomly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression and t-tests!\n",
    "\n",
    "So why are we talking about t-tests?  I thought this was a linear regression class?\n",
    "\n",
    "This is how we test if a predictor is 'significant'.  In a business context maybe we discover that our company's number of hours worked isn't a significant predictor of success; with that information we could argue that taking next Monday off isn't such a bad thing.\n",
    "\n",
    "Enough about stats and business, lets get back to some snails.\n",
    "\n",
    "* With the imported abalone dataset\n",
    "  * Use `Whole_Weight` and `Diameter` as the predictors and `Rings` as the target.\n",
    "  * Build a regression model using `statsmodels`\n",
    "  * Print the model summary\n",
    "* Use the output to identify if any of the predictors are significant.  Which ones are if any? `____________`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    "    \"Sex\",\n",
    "    \"Length\",\n",
    "    \"Diameter\",\n",
    "    \"Height\",\n",
    "    \"Whole_Weight\",\n",
    "    \"Shucked_Weight\",\n",
    "    \"Visecra_Weight\",\n",
    "    \"Shell_Weight\",\n",
    "    \"Rings\",\n",
    "]\n",
    "abalone = pd.read_csv(\n",
    "    \"https://docs.google.com/spreadsheets/d/1GwCnxFT4Sd6iZDj07kNNhEREr7OJQnGvtxd67b5AMio/export?format=csv\",\n",
    "    names=cols,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use `Whole_Weight` and `Diameter` as the predictors\n",
    "X = abalone[[\"Whole_Weight\", \"Diameter\"]]\n",
    "\n",
    "# `Rings` as the target\n",
    "y = abalone[\"Rings\"]\n",
    "\n",
    "# Build a regression model using `statsmodels`\n",
    "X_const = sm.add_constant(X)\n",
    "model = sm.OLS(y, X_const).fit()\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `sklearn` agrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "weight_coef, diameter_coef = model.coef_\n",
    "print(weight_coef, diameter_coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can recreate this output by 'bootstrapping' our data, fitting a model to each bootstrapped sample and seeing how the coefficients change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "data = abalone[[\"Whole_Weight\", \"Diameter\", \"Rings\"]]\n",
    "\n",
    "weight_coefs = []\n",
    "diameter_coefs = []\n",
    "for i in range(1000):\n",
    "    sample = abalone.sample(frac=1.0, replace=True)\n",
    "    X = sample[[\"Whole_Weight\", \"Diameter\"]]\n",
    "    y = sample[\"Rings\"]\n",
    "\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "\n",
    "    weight_coef, diameter_coef = model.coef_\n",
    "\n",
    "    weight_coefs.append(weight_coef)\n",
    "    diameter_coefs.append(diameter_coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_ci_lo = np.percentile(weight_coefs, 2.5)\n",
    "weight_ci_hi = np.percentile(weight_coefs, 97.5)\n",
    "\n",
    "print(\"Weight Coefficient CI\")\n",
    "weight_ci_lo, weight_ci_hi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diameter_ci_lo = np.percentile(diameter_coefs, 2.5)\n",
    "diameter_ci_hi = np.percentile(diameter_coefs, 97.5)\n",
    "\n",
    "print(\"Diameter Coefficient CI\")\n",
    "diameter_ci_lo, diameter_ci_hi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Plot histograms for the coefficient sampling distributions\n",
    "* Add lines for the ci bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "## Afternoon\n",
    "\n",
    "Let's start model Rings from scratch.  We won't restrict ourselves on the features we can use.  Everything is fair game. Readyyyy.. go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "Let's rebuild our model and use an *interaction term* (\\**gasp*\\*).  Let's go back to using only `'Whole_Weight'` and `'Diameter'`.  After creating the `X` and `y`:\n",
    "\n",
    "* Create a new feature that captures the interaction between our current 2 predictors\n",
    "  * Multiply the 2 features together and assign this to a new column\n",
    "* Rebuild the model using the 3 features\n",
    "* Re-print the summary\n",
    "* Interpret the t-tests for these coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X (['Whole_Weight', 'Diameter']) and y ('Rings')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new feature that captures the interaction between our current 2 predictors\n",
    "X[\"ww_d_interaction\"] = _____\n",
    "\n",
    "# Build a regression model using `statsmodels`\n",
    "____\n",
    "\n",
    "# Print the model summary\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait, what did we just do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# No need to worry about this code for needs today.\n",
    "# Of course, give it a look if you're curious.\n",
    "\n",
    "# its easier to see if we bin we'll bin weight and then a model\n",
    "# for diameter when within each bin of weight\n",
    "def bin_array(x, n_bins=5):\n",
    "    percentiles = np.linspace(0, 100, n_bins + 1)[:-1]\n",
    "    bins = np.percentile(x, percentiles)\n",
    "    binned = np.digitize(x, bins)\n",
    "    return binned\n",
    "\n",
    "\n",
    "abalone[\"binned_weight\"] = bin_array(abalone[\"Whole_Weight\"])\n",
    "bins = abalone[\"binned_weight\"].unique()\n",
    "for b in bins:\n",
    "    subset = abalone[abalone[\"binned_weight\"] == b]\n",
    "\n",
    "    # if we have only 1 row we wont fit a model\n",
    "    if subset.shape[0] <= 1:\n",
    "        continue\n",
    "\n",
    "    X = subset[\"Diameter\"]\n",
    "    y = subset[\"Rings\"]\n",
    "\n",
    "    X_const = sm.add_constant(X)\n",
    "    y_pred = sm.OLS(y, X_const).fit().predict()\n",
    "\n",
    "    lab = f\"Weight: Bin {b}\"\n",
    "    plt.plot(X, y_pred, label=lab, lw=4)\n",
    "    ## (un)comment below to toggle plotting on same axes\n",
    "    plt.scatter(X, y, label=lab, alpha=0.1)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plt.xlabel(\"Diameter\")\n",
    "plt.ylabel(\"Rings\")\n",
    "plt.title(\"Interaction between Diameter and Weight\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what do we see here? How Diameter affects Rings depends on how heavy our snail is! When our snail is lighter a higher diameter means more Rings.  When we go up in weight we start to see this trend taper off, at a certain weight diameter starts to matter less for predicting rings! In other words the effects of diameter and weight interact with each other."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
